# Adversarial Attacks
Papers:

Towards Evaluting the Robustness of Neural Networks(CW)

EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES(FGSM)

ADVERSARIAL EXAMPLES IN THE PHYSICAL WORLD(BIM & ILCM)

1: On the Robustness of Semantic Segmentation Models to Adversarial Attacks

2: Adversarial Examples that Fool both Computer Vision and Time-Limited Humans

# Review and Survey about Adversarial Attacks

1. Adversairal Examples in Modern Machine Learning: A Review

2. Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey

3. Review of Artificial Intelligence Adversairal Attack and Defense Technologies

# Universal Adversairal Perturbation

1. Universal Adversarial Perturbations

2. Generative Adversarial Perturbations

3. Syntheszing Robust Adversarial ExamplesS(EOT)

4. Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition

5. Universal Adversarial Perturbations Against Semantic Image Segmentation

6. G-UAP: Generic Universal Adversarial Perturbation that Fools RPN-based Detectors

7: Fast-UAP: Algorithm for Seepding up Universal Adversarial Perturbation Generation with Orientation of Perturbation Vectors

8: Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions

9: Generalizable Data-free Objective for Crafting Universal Adversarial Perturbations

# Patch Attacks

1: Adversarial Patch(The First Patch)

2: DPATCH: An Adversarial Patch Attack on Object Detectors

3: On Physical Adversarial Patches for Object Detection

4: Adversarial Patches Exploitiong Contextual Reasoning in Object Detection

5: UPC: Learning Universal Physical Camouflage Attacks on Object Detection

6: HIDING OBJECTS FROM DETECTORS: EXPLORING TRANSFERRABLE ADVERSARIAL PATTERNS

7: Adversarial Examples that Fool Detectors

8: ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector

9: Robust Physical-World Attacks on Deep Learning Visual Classification

10: Physical Adversarial Examples for Object Detectors

# Adversarial Defense

1: ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples

2: MagNet: a Two-Pronged Defense against Adversarial Examples

3: DEFENSE-GAN: PROTECTING CLASSIFIERS AGAINST ADVERSARIAL ATTACKS USING GENERATIVE MODELS

# Video Perturbation

1: Sparse Adversarial Perturbations for Videos

2: Adversarial Perturbations Against Real-Time Video Classification Systems

3: Stealthy Adversarial Perturbetion Against Real-Time Video Classification Systems

# Video Recognition or Detection

1: Heuristic Black-box Adversarial Attacks on Videos Recognition Models

2: Flow-Guided Feature Aggregation for Video Object Detection(FGFA)

3: Deep Feature Flow for Video Recognition

4: Optimizing Video Object Detection via a Scale-Time Lattice

# Others

1: Improving Transferability of Adversarial Examples with Input Diversity

2: Adversarial Examples Improve Image Recognition

3: Real-Time Semantic Segementation via Multiply Spatial Fusion Network

